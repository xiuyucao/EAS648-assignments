---
title: "Change Analyses"
author: "[Xiuyu Cao](https://github.com/xiuyucao)"
date: "Nov 6, 2023"
output: html_document
---

******************************************************
## Introduction
In this study, I want to find out the trend in using the social media platform [Flickr](https://www.flickr.com), as well as the users preference of sharing whether photos of nature or not. The sharing pattern of nature photos may indicate some weather events

******************************************************
## Needed Packages
```{r import packages, message=F}
library(dplyr)  # for working with dataframes
library(lubridate)  # for working with time data
library(ggplot2)  # for plotting
library(gganimate)  # for plotting animated figures
```

******************************************************
## Data Preparing
The social media data used here is from [Flickr](https://www.flickr.com). They are photos taken in Michigan in recent twenty years. First read the data.
```{r read data}
MichFlickr <- read.csv('../data/lab3/MichiganFlickr.csv')  # read the csv file
head(MichFlickr)  # show the first few lines of the data
```

As we can see, the `dateupload` attribute is a huge integer. It represents the upload time in seconds from 1970-01-01. We need to convert it to a legible format using the functions in the `lubridate` package. The `as.POSIXct()` function can be used to turn the continuous seconds into a legible time format.
```{r handle datetime data}
MichFlickr$time <- as.POSIXct(MichFlickr$dateupload, origin='1970-01-01')  # change seconds into a legible time data
MichFlickr$date <- as.Date(format(MichFlickr$time, format='%Y-%m-%d'))
# separately create year, month, day, hour, and minute attribute
MichFlickr$year <- year(MichFlickr$time)
MichFlickr$month <- month(MichFlickr$time, label = TRUE)
MichFlickr$day <- day(MichFlickr$time)
# MichFlickr$hour <- hour(MichFlickr$time)
# MichFlickr$minute <- minute(MichFlickr$time)
```
The `predict_Na` attribute gives an index of how possible the picture is a picture of nature. Let's say if `predict_Na`$>0.6$, then it is likely to be a photo of nature.
```{r get whether nature}
# set nature threshold and create an attribute telling whether this is a photo of the nature
MichFlickr$Nature<- MichFlickr$predict_Na > 0.6
```
Also, for future spatial analyses, I need the Michigan's border data. This can be downloaded through the `map_data()` function in the `ggplot2` package.
```{r get MI border}
states <- map_data("state")  # get the state data
mich <- subset(states, region == "michigan")  # subset the states to get only Michigan
counties <- map_data("county")  # get the counties data
mich_county <- subset(counties, region == "michigan")  # subset the counties to get only thoes in Michigan
# get the map
pmich <- ggplot(data=mich,
                mapping=aes(x=long, y=lat, group=group)) +  # set xy and group the peninsulas to prevent wierd lines
  coord_fixed(1.3) +  # make the x 1.3 times greater than y
  geom_polygon(color='black', fill='skyblue') +  # set the border and fill color
  theme_void() +  # get rid of the lat and lon grids
  geom_polygon(data = mich_county, fill = NA, color = "white") +  # set the counties border color
  geom_polygon(color = "black", fill = NA) + # get the state border back on top
  ggtitle('Map of Michigan')
pmich +
  theme(plot.title = element_text(size = 20, color = "black", face='bold', hjust=0.5))  # set title format
```

******************************************************
## Noise Reduction
Social media data include various sources of noise related to the frequency of sharing photographs that can obscure meaningful patterns. Here I will develop a technique for noise reduction that surpasses basic filtering methods. The steps are as follows.

1. Time Range Check
2. Robot Detection
3. (Optional) Remove duplicate posts from one user

### 1 Time Range Check
According to the [Wikipedia page of Flickr](https://en.wikipedia.org/wiki/Flickr), it is a platform launched on Feb 10, 2004. So the posts before this time must be noise and need to be eliminated. Therefore, I will set a time threshold. The `lubridate::as.Date()` function can be used to turn character variable into date time variable.
```{r time range check}
denoise_mich <- MichFlickr %>%
  filter(time >= as.Date('2004-2-10'))  # mask out posts earlier than 2004-2-10
```

### 2 Robot Detection
The analysis of social media needs to address the issue of fake or robot accounts that can add to noise and bias in the result. Here I detect robot accounts based on the account's post behavior: whether is posting contents with a fixed interval.
```{r robot detection}
user_interval <- denoise_mich %>%  # get interval information
  group_by(owner) %>%  # group records from the same owner
  filter(n()>=20) %>%  # find robots from users who have posted >=20 times
  arrange(owner, time) %>%  # sort the records by owner and time
  mutate(interval=c(0,diff(time)))  # compute time interval (sec) in each group (give 0 to the first post)
interval_stats <- user_interval %>%  # get interval statistics
  summarise(publish_count=n(),  # get publish count
            interval_pattern = as.numeric(names(which.max(table(interval)))),  # most frequent interval duration
            interval_freq=as.numeric(table(interval)[which.max(table(interval))]))  # get same interval frequency
head(interval_stats)
```
Human accounts should not have a strong pattern of posting. If most of the interval between each post is the same, the account is more likely to be a robot. Here I would say if an account has more than 60% of its contents are posted with same interval, it is a robot.
```{r kill the robot}
robot <- interval_stats %>%  # get robot blacklist
  filter(interval_freq/publish_count>0.6 & interval_pattern>1) %>%  # 60% same interval and the interval > 1 sec
  arrange(desc(publish_count))  # sort the records by publish count
head(robot)
# now do anti join to delete the robot accounts in the original data
denoise_mich <- anti_join(denoise_mich, robot, by='owner')  # remove robot accounts
nrow(MichFlickr)-nrow(denoise_mich)  # compute how many are removed
```
`r nrow(MichFlickr)-nrow(denoise_mich)` records, which are likely to be noise, are deleted.

### 3 (Optional) Remove duplicate posts from one user
When analyzing the the trend in using the social media platform, we wanna find out how many people are posting contents in Flickr. So we do not want to take into account one user have multiple posts within a time period we set.
```{r repetition detection}
# function of repetition detection
rm_rep_post <- function(df,time_range){
if (time_range == 'd') {
    df <- df %>%
      group_by(owner, date) %>%
      slice(1) %>%
      ungroup()
  } else if (time_range == 'm') {
    df <- df %>%
      group_by(owner, year, month) %>%
      slice(1) %>%
      ungroup()
  } else if (time_range == 'y') {
    df <- df %>%
      group_by(owner, year) %>%
      slice(1) %>%
      ungroup()
  } else {
    warning("Invalid time range specified. No grouping performed.")
  }

  return(df)
}
```

******************************************************
## Data Analysis
### 1 Flickr Popularity Trend
Analyzing the evolution of creator number of a social media is important because it identifies periods of growth and decline, providing insights into the platform's overall health.
```{r Flikr popularity trend anlaysis1}
denoise_mich$count <- 1 # create a count attribute for adding up
# analyze daily trend
daily_creator <- denoise_mich %>%
  rm_rep_post('d') %>%
  group_by(date) %>%
  summarise(total_creator=sum(count)) %>%
  na.omit()
head(daily_creator)
```
Using `ggplot()`, we can plot the time series data.
```{r}
# plot
p <- ggplot(daily_creator, aes(x = date, y = total_creator)) +  # set x and y axes
  geom_line(color = "#00AFBB", linewidth = 1)  # plot line
  scale_x_date(date_labels = "%b")  # set date labels to abbreviation of months
# Smooth the data based on a spline function
p + geom_smooth(method = lm, formula = y ~ splines::bs(x, 3), se = FALSE)
```

Predict
```{r}
creator_ts <- ts(daily_creator$total_creator,start=c(2004,3),end=c(2018,3),frequency=365)
plot(creator_ts)
library(forecast)
se_mod <- ses(creator_ts, h = (3*365))
plot(se_mod)

arima_model <- auto.arima(creator_ts)
plot(arima_model$x,col="red")
lines(fitted(arima_model),col="blue")
fore_arima = forecast::forecast(arima_model, h=(3*365))
plot(fore_arima,ylim=c(0,125))
```




```{r Flikr popularity trend anlaysis2}
# analyze monthly trend
monthly_creator <- denoise_mich %>%
  rm_rep_post('m') %>%
  group_by(year,month) %>%
  summarise(total_creator=sum(count)) %>%
  na.omit()
head(monthly_creator)
```

```{r}
# plot
monthly_creator  %>%
    ggplot(aes(x = month, y = total_creator, group = year)) +
    geom_line(aes(color = as.factor(year))) +
    scale_color_discrete() + 
    labs(title = "Total Flickr Photographs for Michigan", x = "", y = "Total Photographs",
         subtitle = "Activity is highest for the summer months") +
   theme_classic()
```

```{r}
# yearly creator
yearly_creator <- denoise_mich %>%
  rm_rep_post('y') %>%
  group_by(year) %>%
  summarise(total_creator=sum(count)) %>%
  na.omit()
head(yearly_creator)
#plot
p <- ggplot(yearly_creator, aes(x = year, y = total_creator)) +  # set x and y axes
  geom_line(color = "#00AFBB", linewidth = 1)  # plot line
p
```

There was a tornado in Michigan in 2012, I wanna find out the impact of weather on users' sharing nature photos.
```{r animate the data}
animateMich <- denoise_mich %>%
  filter(date >= as.Date('2012-03-01') & date <= as.Date('2012-3-31') & Nature==T)
p3 <-pmich + theme_void() + 
        geom_polygon(data = mich_county, fill = NA, color = "white") +
        geom_polygon(color = "black", fill = NA) + 
        geom_point(data = animateMich, aes(longitude, latitude), inherit.aes = FALSE) +
        labs(title = 'Date: {format(frame_time, "%b %d %Y")}') +
        transition_time(date) 

animate(p3 + shadow_wake(0.1), fps=5)
```






