---
title: "Applied Spatial Analysis"
author: "[Xiuyu Cao](https://github.com/xiuyucao)"
date: "Nov 22, 2023"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---

******************************************************
## Needed Packages
```{r import needed packages, message=F}
library(terra)  # for working with raster data
library(tidyverse)  # for manipulating the data
library(ape)  # for getting autocorrelation coefficient
library(reshape2)  # for reshaping data frames
library(ROCR)  # for getting the ROC curve
```

******************************************************
## Introduction
* Spatial Analysis simplifies the huge amount of detailed information in order to extract the main trends
* Spatial dependency leads to spatial autocorrelation, which violates standard statistical techniques that assume independence among observations
  * Positive Spatial Autocorrelation
  * Negative Spatial Autocorrelation
* spatial regression models capture the relationships and do not suffer from these weaknesses, but are computationally intensive
* It is also appropriate to view spatial dependency as a source of information rather than something to be corrected
* Spatial Autocorrelation Statistics measure and analyze the degree of dependency among observations in a geographic space.


******************************************************
## Salt Lake City
In recent decades, Salt Lake City, Utah, has experienced significant urban development, marked by substantial population growth, immigration, and increased housing demand. This expansion has led to a noteworthy increase in impervious urban development. To facilitate effective land management and conservation efforts, it is crucial to study the factors contributing to urban development and its trends.

### Read Data
First read in the data I will be using to analyze Salt Lake City. They are raster data of the same resolution and extent. Based on that, they can be combined into a list using the function `terra::c()`, and further turned into a data frame.
```{r read data}
# read data
NLCD_2001 <- rast("../data/lab5/NLCD_2001_SL.tif")  # land use data 2001
NLCD_2004 <- rast("../data/lab5/NLCD_2004_SL.tif")  # land use data 2004
NLCD_2006 <- rast("../data/lab5/NLCD_2006_SL.tif")  # land use data 2006
NLCD_2008 <- rast("../data/lab5/NLCD_2008_SL.tif")  # land use data 2008
NLCD_2011 <- rast("../data/lab5/NLCD_2011_SL.tif")  # land use data 2011
NLCD_2013 <- rast("../data/lab5/NLCD_2013_SL.tif")  # land use data 2013
NLCD_2016 <- rast("../data/lab5/NLCD_2016_SL.tif")  # land use data 2016
Park_dist <- rast("../data/lab5/Parks_dist_SL.tif")  # distance (km) to parks and the protected
Rd_dns1km <- rast("../data/lab5/Rd_dns1km_SL.tif")  # road density for a 1 km neighborhood
WaterDist <- rast("../data/lab5/WaterDist_SL.tif")  # distance (km) to water bodies
DEM <- rast("../data/lab5/DEM_SL.tif")  # elevation

# stack the raster layers
allrasters <- c(NLCD_2001, NLCD_2004, NLCD_2006, NLCD_2008, NLCD_2011, NLCD_2013, NLCD_2016, Park_dist, Rd_dns1km, WaterDist, DEM)
allrasters[[1]]  # check the data
plot(allrasters[[1]])  # check the data
# turn raster layers into a data frame
allrasters.df <- allrasters %>%
  as.data.frame(xy=T) %>%  # transform to a data frame, keep the xy
  filter(NLCD_2001_SL != 128)  # remove no data value (stored as 128)
head(allrasters.df)
```

### Statistical Analysis
From 2001 to 2016, many areas in Salt Lake City have changed to urban areas.
```{r get is_changed and plot areas changed to urban}
allrasters.df <- allrasters.df %>%  # get is_changed: whether changed to urban area from 2001 to 2016
  mutate(is_changed = (NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) & (NLCD_2016_SL == 21 | NLCD_2016_SL == 22  | NLCD_2016_SL == 23 | NLCD_2016_SL == 24))
# plot
ggplot(allrasters.df, aes(y=y, x=x, color=is_changed)) +
  geom_point(size=2, shape=15) +
  scale_color_manual(values = c("FALSE" = "snow3", "TRUE" = "yellow")) +
  labs(title='Areas Changed to Urban in Salt Lake City, 2001-2016',
       x='X (m)',y='Y (m)', color='Whether Changed')
```

From the map we can see the significant changes in Salt Lake City from 2001 to 2016. Let's analyze this quantitatively.

`as.numeric()` converts the logical vector obtained in the previous step to a numeric vector, where TRUE is converted to 1 and FALSE is converted to 0. They can thus be added up using the `sum()` function and get the count of the cells.
```{r quantitatively analyze urban change}
# get the new urban areas after 2001
urban_new <- with(allrasters.df, (sum(as.numeric(NLCD_2016_SL == 21 | NLCD_2016_SL == 22 | NLCD_2016_SL == 23 | NLCD_2016_SL == 24))) - (sum(as.numeric(NLCD_2001_SL == 21| NLCD_2001_SL == 22| NLCD_2001_SL == 23| NLCD_2001_SL == 24))))
# get original urban areas in 2001
urban_ori <- with(allrasters.df,(sum(as.numeric(NLCD_2001_SL == 21| NLCD_2001_SL == 22| NLCD_2001_SL == 23| NLCD_2001_SL == 24))))
# get percentage change
urban_new/urban_ori* 100
```

Urban areas increased by About `r round(urban_new/urban_ori* 100,2)`% in Salt Lake City from 2001 to 2016, this is huge!

Urbanization is necessary for more economic opportunities and development of infrastructure for the well being of citizens, while it can cause various consequences including the habitat loss of many species and Urban Heat Island effect. Therefore, it is an important land management problem to decided where to develop new urban area and predict the urban area development to plan future land management and conservation. Let's quantitatively analyze where these changes took place during 2001-2016.
```{r calculate distances changed, message=F}
# get distance to different areas
data2plot <- allrasters.df %>%
  filter(NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) %>%  # get potential areas to develop in 2001
  select(Parks_dist_SL:is_changed) %>%  # select the last columns
  melt()  # turn into long format for plot

# set plot legend labels
legend_labels <- c('Distance to Green Areas (km)',
                   'Road Density (1km Neighborhood)',
                   'Distance to Water Bodies (km)',
                   'Elevation (m)')
# plot
ggplot(data2plot, aes(x=is_changed, y=value,fill=variable)) +  # set plot
  scale_fill_manual(values = c('green3','gray','steelblue1','yellow'),  # set fill color
                    labels=legend_labels) +  # set legend names
  geom_boxplot() +  # set box plot
  facet_wrap(~variable, scales='free_y') +  # different plots
  labs(x="Whether Changed to Urban Area", y='Value') +  # set x y labels
  guides(fill = guide_legend(title = NULL)) +  # remove legend title
  theme(strip.text = element_blank()) +  # remove subplot titles
  ggtitle('Where Urbanization Took Place in Salt Lake City, 2001-2016')  # set title
```

As shown in the plot, in Salt Lake City during 2001-2016, Urban Development followed these patterns:

* farther from the green area (parks and protected areas)
* more prevalent in areas with higher road density
* more prevalent in areas with lower elevation

These patterns make sense. For future land management and conservation, a prediction of probable developing area is needed.

### Urban Development Model
To forecast future urban development in Salt Lake City, I will develop a predictive model using the areas that were not urban in 2001 and to see what contributed to the urban change during 2001-2016.

#### Sampling
Due to the huge amount of the data, a random sample is needed to do the further analysis. Here my random seed is set to 77. Since there are many non-urban areas and few urban areas. I will sample all of the area changed to urban and some of the areas not changed during 2001 to 2016, making them 1:2
```{r get random sample of the whole data set}
set.seed(77)  # set a random seed
# set.seed(NULL)

# get all of the area that are not urban in 2001
non_urban01 <- filter(allrasters.df,
                   NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24)
# separately get the area changed and not changed to urban
sl_chg <- filter(non_urban01, is_changed == T)  # changed
sl_nchg <- filter(non_urban01, is_changed == F)  # not changed
# get sample
sample_index <- sample(1:nrow(sl_nchg), nrow(sl_chg)* 2)  # get sample index
sl_sample <- rbind(sl_chg, sl_nchg[sample_index,])  # combine the area changed and not changed
```

To assess my sampling result, first check the histogram of the original data and the sampled data.
```{r compare histogram}
par(mfrow=c(2, 5),  # set 2*5 sub plots
    mar=c(4, 4, 0.8, 0.65))  # set bottom, left, top, right margin

title=c('LandUse16','Park Dist', 'Road Dense', 'Water Dist', 'DEM')  # set title
for (i in 9:13) {  # plot original on the first row
  hist(non_urban01[, i], main=title[i-8], xlab=NA, col='plum1', ylab=ifelse(i==9,'Frequency (Original)',NA))
}
for (i in 9:13){  # plot sampled on the second row
  hist(sl_sample[, i], main=NA, xlab="Value", col='skyblue', ylab=ifelse(i==9,'Frequency (Sampled)',NA))
}

par(mfrow=c(1, 1))  # set graphic parameter to normal
```
As shown in the graph, the distribution of the original data closely mirrors that of the sampled data, indicating an unbiased sample that can represent the original data distribution.

Also, it is important to check the spatial dependency.
```{r assess spatial dependency}
sd_sample <- sl_sample[sample(1:nrow(sl_sample), 100),]  # randomly get 100 records for checking the spatial dependency
dist_mat <- as.matrix(dist(cbind(sd_sample$x, sd_sample$y)))  # get distance matrix
dist_mat.i <- 1/dist_mat  # get reciprocal distance matrix
diag(dist_mat.i) <- 0  # set diagonal to 0

raster_names <- names(sd_sample)[3:ncol(sd_sample)]  # get raster names
raster_names <- data.frame(Raster_Name = raster_names)  # create data frame with raster names being the first column
Moran_res <- data.frame()  # create blank data frame
for(i in 3:ncol(sd_sample)){  # get each record
  Moran_res <- rbind(Moran_res, Moran.I(sd_sample[,i], dist_mat.i))
}
cbind(raster_names, Moran_res)  # combine the to data frames by column
```
From the result, the expected value is -0.01. The observed values of each raster layer vary.

* The `sd` and `p` are all small, showing a good precision and significance in the result.
* For the land use type, most of their Moran's Indexes are about or below 0.1 and above 0, indicating very weak positive spatial autocorrelation.
* For the distance to park, road density, distance to water bodies, and elevation their Moran's I range from 0.2 to 0.4, indicating a moderate to strong positive spatial autocorrelation, meaning that there is some level of clustering or pattern in these spatial data.

#### Model Development
The Generalized Linear Model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error non-normal distributions. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. Here I use the `glm` function to call the GLM, and set `family=binomial`, meaning that it is fitting a logistic regression model since our `is_changed` attribute is binary with only the value `True` or `False`.
```{r get train and test set and train the model}
# get train (70%) and test (30%) set
sl_sample <- sl_sample %>% mutate(id = row_number())  # add row number
sl_train <- sl_sample %>% sample_frac(.7)  # get train set
sl_test <- anti_join(sl_sample, sl_train, by='id')  # get test set
# get model using the train set
fit <- glm(is_changed ~ Parks_dist_SL + Rd_dns1km_SL + WaterDist_SL + DEM_SL, data=sl_train, family=binomial())
summary(fit)
```
As shown in the result, the `Estimate` shows the weight of influence on the independent variable. From the values, we can conclude that the distance to the parks, road density, and distance to the water bodies all have a positive relationship with urban development, while the elevation has a negative one. The `Std. Error` are all small, indicating the small uncertainty of the estimate. The `z value` are all greater than 2 when positive or smaller than -2 when negative, showing that the observed result is unlikely to have occurred by chance alone. The `Pr(>|z|)` all being very small also indicates the significance in the model.

The null deviance shows how well the response variable is predicted by a model that includes only the intercept. The residual deviance shows how well the response is predicted by the model when the predictors are included. It can be seen that the deviance goes down by 280958 - 115555 = `r 280958-115555` when 4 predictor variables are added. This decrease in deviance is evidence of a significant increase in fit.

To further assess the goodness of fit, I will calculate the Revceiver Operating Characteristic (ROC) curve using the test set.
```{r get ROC}
pred <- prediction(predict(fit, newdata=sl_test), sl_test$is_changed)  # use the test set to calculate the ROC curve
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE, main = "ROC Curve")
```

From the ROC curve we can see that the ROC curve rises steeply and approaches the upper-left corner, indicating high sensitivity and specificity. To quantitatively assess the curve, the Area Under the Curve (AUC) needs to be calculated.
```{r get AUC}
# get AUC value
auc_ROCR <- performance(pred, measure = "auc")
auc_ROCR@y.values[[1]]
```
The AUC is about 0.96, showing that the model is performing exceptionally well in terms of separating true positives from false positives and true negatives from false negatives, likely to make accurate predictions as well.

#### Prediction
Use the whole data set to generate the final model and predict the urban development.
```{r predict development}
fit <- glm(is_changed ~ Parks_dist_SL + Rd_dns1km_SL + WaterDist_SL + DEM_SL, data=sl_sample, family=binomial())  # get model
predicted <- predict(allrasters, fit)  # get prediction
predicted <- predicted %>%
  as.data.frame(xy=T) %>%  # change to data frame
  mutate(probability=plogis(lyr1))  # get probability

# plot
ggplot(predicted, aes(y=y, x=x, color=probability)) +
  geom_point(size=2, shape=15) +
  scale_color_gradient(low = "snow3", high = "yellow") +
  labs(title='Likely Locations of Urban Development, Salt Lake City',
       x='X (m)',y='Y (m)')
```

### Conclusion
Salt Lake City has undergone huge urban development. From the results of the statistical analysis and the GLM, we can see the distance to the parks, road density, and distance to the water bodies all have a positive relationship with urban development, while the elevation has a negative one. And the road density contribute the most to the urban development. In order to better explain urban development, besides the land-use type, I can add other data such as:

* population density: High population density can be associated with increased urbanization. It reflects the concentration of people in a given area.
* economic indicators: Variables related to economic activities, such as employment rates, income levels, and economic growth
* environmental factors: air quality (Aerosol Optical Depth by remote sensing images), Urban Heat Island effect (Land Surface Temperature by Landsat), etc.
* other data like Nighttime Light Remote Sensing Imagery, which can be an indicator of the urban development.

******************************************************
## Cluster Analysis

```{r}
library(fastcluster)
library(graphics)
library(ggdendro)
```

### Data Read-in
```{r read amenity data}
library(sf)  # for working with simple feature data
amenData<- st_read("../data/lab5/AmenDataAll.shp")
head(amenData)
names(amenData)
```

```{r store the geometry}
geomData <- st_sfc(amenData$geometry)
```

### Choose Data
```{r transforming the data}
amenData$ZooEmpDist_log <- log(amenData$ZooDistEmp + 0.00000001)
amenData$HotelEmpDist_log <- log(amenData$HotelDistE + 0.00000001)
amenData$HistMonDist_log <- log(amenData$HistMon_Di + 0.00000001)
amenData$HighEdEmpDist_log <- log(amenData$HigherEdDi + 0.00000001)
amenData$GolfEmpDist_log <- log(amenData$GolfDistEm + 0.00000001)

amenData$SocialNorm <- amenData$Nat_Flickr/(amenData$serPop10 + 1)
amenData$HousingChg <- amenData$Urb2011 - amenData$Urb2001
```
choose variables based on a thematic inquiry of my choice
```{r choose the data}
amenDataDF<-amenData[,c("SocialNorm", "HousingChg", "Frst2011", "WaterPct", "distcoast", "DEM_max","DEM_range", "HikeLength","ZooEmpDist_log", "HotelEmpDist_log","HistMonDist_log", "HighEdEmpDist_log", "GolfEmpDist_log")]
## make sure there are no missing data
amenDataDF <- na.omit(amenDataDF)
## we need to make it into a normal dataframe to 
## do the analysis
amenDataDF <- as.data.frame(amenDataDF)[1:12]
## calculate z-scores for the dataset
db_scaled <- scale(amenDataDF)
```

### Cluster
visualize and provide qualitative descriptions of the groups derived from the clustering techniques
provide evidence of my characterization
give an explanation of the spatial distribution of these groups
```{r}
# Determine number of clusters
wss <- (nrow(db_scaled)-1)*sum(apply(db_scaled,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(db_scaled,
   centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")
```

```{r kmeans}
# K-Means Cluster Analysis
fit <- kmeans(db_scaled, 5) # 5 cluster solution
# get cluster means
aggregate(db_scaled,by=list(fit$cluster),FUN=mean)
# append cluster assignment
amenData <- data.frame(amenData, fit$cluster)
st_geometry(amenData) <- geomData
# plot
ggplot() + 
  geom_sf(data = amenData, mapping = aes(fill = as.factor(fit.cluster)), color = NA) + 
  theme(legend.position = "none") +
  ggtitle("Clusters based on Kmeans")
```

