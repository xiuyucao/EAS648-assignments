---
title: "Applied Spatial Analysis"
author: "[Xiuyu Cao](https://github.com/xiuyucao)"
date: "Nov 18, 2023"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---

******************************************************
## Needed Packages
```{r import needed packages, message=F}
library(terra)  # for working with raster data
library(tidyverse)  # for manipulating the data
library(ape)  # for getting autocorrelation coefficient
library(reshape2)  # for reshaping data frames
```

******************************************************
## Introduction
* Spatial Analysis simplifies the huge amount of detailed information in order to extract the main trends
* Spatial dependency leads to spatial autocorrelation, which violates standard statistical techniques that assume independence among observations
  * Positive Spatial Autocorrelation
  * Negative Spatial Autocorrelation
* spatial regression models capture the relationships and do not suffer from these weaknesses, but are computationally intensive
* It is also appropriate to view spatial dependency as a source of information rather than something to be corrected
* Spatial Autocorrelation Statistics measure and analyze the degree of dependency among observations in a geographic space.


******************************************************
## Salt Lake City
In recent decades, Salt Lake City, Utah, has undergone considerable urban development characterized by substantial population growth, immigration, and heightened housing demand. This expansion has resulted in a noteworthy 17.6 percent rise in impervious urban development from 2002 to 2010.

### Read Data
First read in the data I will be using to analyze the Salt Lake City. They are raster data of the same resolution and extent. Based on that, they can be combined into a list using the function `terra::c()`, and further turned into a data frame.
```{r read data}
# land use data
NLCD_2001 <- rast("../data/lab5/NLCD_2001_SL.tif")
NLCD_2004 <- rast("../data/lab5/NLCD_2004_SL.tif")
NLCD_2006 <- rast("../data/lab5/NLCD_2006_SL.tif")
NLCD_2008 <- rast("../data/lab5/NLCD_2008_SL.tif")
NLCD_2011 <- rast("../data/lab5/NLCD_2011_SL.tif")
NLCD_2013 <- rast("../data/lab5/NLCD_2013_SL.tif")
NLCD_2016 <- rast("../data/lab5/NLCD_2016_SL.tif")
# distance to parks and protected areas (km)
Park_dist <- rast("../data/lab5/Parks_dist_SL.tif")
# road density for a 1 km neighborhood
Rd_dns1km <- rast("../data/lab5/Rd_dns1km_SL.tif")
# distance to water bodies in km (Euclidean)
WaterDist <- rast("../data/lab5/WaterDist_SL.tif")
# elevation
DEM <- rast("../data/lab5/DEM_SL.tif")
# combine the raster layers
allrasters <- c(NLCD_2001, NLCD_2004, NLCD_2006, NLCD_2008, NLCD_2011, NLCD_2013, NLCD_2016, Park_dist, Rd_dns1km, WaterDist, DEM)

allrasters[[1]]  # check the data
plot(allrasters[[1]])  # check the data

# turn it into a data frame
allrasters.df <- allrasters %>%
  as.data.frame(xy=T) %>%  # transform to a data frame
  filter(NLCD_2001_SL != 128)  # remove no data value (stored as 128)
head(allrasters.df)
```

### Sampling
```{r show how many rows}
nrow(allrasters.df)  # get the row number of the data frame
```
Because of the huge amount of data, a random sample is needed to do the further analysis.

Using the function `spatSample()` in the `terra` package, a random sample can be deployed by setting the `method` parameter to `random`. Here I sample 100 points for the Salt Lake region.
```{r random sample}
sampleSLrnd <- allrasters %>%
  spatSample(size=100, method="random", cells=TRUE, xy=TRUE) %>%  # do random sample
  filter(NLCD_2001_SL != 128)  # remove no data value
# plot sample points
plot(sampleSLrnd$x, sampleSLrnd$y,
     main='Random Sample points', xlab='X (m)', ylab='Y (m)')
```

### Accessing the Dataset
To assess my sampling result, first check the histogram of the original data and the sampled data.
```{r compare histogram}
par(mfrow=c(2, 5),  # set 2*5 sub plots
    mar=c(4, 4, 0.8, 0.65))  # set bottom, left, top, right margin

title=c('LandUse16','Park Dist', 'Road Dense', 'Water Dist', 'DEM')  # set title

for (i in 9:13) {  # plot orignal on the first row
  hist(allrasters.df[, i], main=title[i-8], xlab=NA, col='plum1', ylab=ifelse(i==9,'Frequency (Original)',NA))
}
for (i in 9:13){  # plot sampled on the second row
  hist(sampleSLrnd[, i+1], main=NA, xlab="Value", col='skyblue', ylab=ifelse(i==9,'Frequency (Sampled)',NA))
}

par(mfrow=c(1, 1))  # set graphic parameter to normal
```

Also, I want to check the spatial autocorrelation. To assess spatial dependency. First get distance matrix, then assess the specific spatial autocorrelation using Moran’s I, which is the most widely use.
```{r assess spatial dependency}
dist_mat <- as.matrix(dist(cbind(sampleSLrnd$x, sampleSLrnd$y)))  # get distance matrix
dist_mat.i <- 1/dist_mat  # get reciprocal distance matrix
diag(dist_mat.i) <- 0  # set diagonal to 0

raster_names <- names(sampleSLrnd)[4:ncol(sampleSLrnd)]
raster_names <- data.frame(Raster_Name = raster_names)
Moran_res <- data.frame()
for(i in 4:ncol(sampleSLrnd)){
  Moran_res <- rbind(Moran_res, Moran.I(sampleSLrnd[,i], dist_mat.i))
}
cbind(raster_names, Moran_res)
```
From the result, the expected value is -0.01 while the observed value is 0.19, indicating the moderate positive spatial autocorrelation. Similar values are clustered together in space

A Moran's I value of 0.19 indicates positive spatial autocorrelation, meaning that similar values are clustered together in space. The Moran's I statistic ranges from -1 to 1, where:

If Moran's I is close to 1, it suggests strong positive spatial autocorrelation.
If Moran's I is close to -1, it suggests strong negative spatial autocorrelation.
If Moran's I is close to 0, it suggests spatial randomness.
In your case, with a Moran's I value of 0.19, the positive sign indicates that nearby locations tend to have similar values. The magnitude (0.19) suggests a moderate degree of spatial autocorrelation. Researchers or analysts would typically interpret this value in the context of the specific study and the characteristics of the data. It implies that there is some level of clustering or pattern in the spatial distribution of the variable being analyzed.

### Statistical Analysis
identify the pixels that were not urban area in 2001 but have changed to urban area by 2016.
```{r}
allrasters.df <- allrasters.df %>%  # get urbanChg: whether changed to urban area from 2001 to 2016
  mutate(urbanChg = (NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) & (NLCD_2016_SL == 21 | NLCD_2016_SL == 22  | NLCD_2016_SL == 23 | NLCD_2016_SL == 24))
# plot
ggplot(allrasters.df, aes(y=y, x=x, color=urbanChg)) +
   geom_point(size=2, shape=15) +
   theme()
```

**********************************************************************
How much change did Salt Lake experience from 2001 to 2016:
`as.numeric()` converts the logical vector obtained in the previous step to a numeric vector, where TRUE is converted to 1 and FALSE is converted to 0.
`sum()` calculates the sum of the numeric vector obtained in the previous step
```{r}
## calculate total new urban impervious for 2016
newUrban <- with(allrasters.df,(sum(as.numeric(NLCD_2016_SL == 21 | NLCD_2016_SL == 22 | NLCD_2016_SL == 23 | NLCD_2016_SL == 24))) - (sum(as.numeric(NLCD_2001_SL == 21| NLCD_2001_SL == 22| NLCD_2001_SL == 23| NLCD_2001_SL == 24))))
## calculate total urban impervious for 2001
urban2001 <- with(allrasters.df,(sum(as.numeric(NLCD_2001_SL == 21| NLCD_2001_SL == 22| NLCD_2001_SL == 23| NLCD_2001_SL == 24))))
## percentage increase in urban impervious
newUrban/urban2001* 100
```

compare means and plot the variance between newly developed urban areas and the different variables. Let’s look at the influence of the distance to protected areas.

Let’s look at the mean and variance of the variables that might shape where there is new development. We have to reshape the data to achieve this.
```{r}
sl_plot <- allrasters.df %>%
  filter(NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) %>%  # get not urban area
  select(Parks_dist_SL:urbanChg) %>%  # select the last columns
  melt()  # turn into long format with urbanChg, varable, and value

# plot
ggplot(sl_plot, aes(x=urbanChg, y=value,fill=variable))+
  geom_boxplot()+
  facet_grid(.~variable)+
  labs(x="X (binned)")+
  theme(axis.text.x=element_text(angle=-90, vjust=0.4,hjust=1))
```

### General Linear Models
The generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error non-normal distributions. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.

Ordinary linear regression predicts the expected value of a given unknown quantity (the response variable, a random variable) as a linear combination of a set of observed values (predictors). This implies that a constant change in a predictor leads to a constant change in the response variable (i.e. a linear-response model). This is appropriate when the response variable has a normal distribution (intuitively, when a response variable can vary essentially indefinitely in either direction with no fixed “zero value”, or more generally for any quantity that only varies by a relatively small amount, e.g. human heights).

Use The sample() function draw a random number indexes representing the rows of a dataframe. In cases where we have many absence data (area that were not develop) and few presence (developed), a common procedure is to retain all presence data and taking a smaller randomly sample of the absence. A ratio of 1:2 is common, but this is not a rule, and other ratios can be used and are often tested.
```{r}
###Grab all the developed cells (presence)
newUrban <- SL %>%
  filter(urbanChg == TRUE)

###Grab all the nondeveloped and not previously urban cells (absence)
nonUrban <- SL %>%
  filter(urbanChg == FALSE)

###Get a random sample of the absence data
### that is twice as large as the presence data
index <- sample(1:nrow(nonUrban), (round(nrow(newUrban)* 2)))
SLsampleUrban <- nonUrban[index, ]

### combine the orginal presence and absence data
SLsample2 <- cbind(SLsampleUrban, newUrban)
SLsample <- rbind(SLsampleUrban, newUrban)

###Consider a train and testing sample by futher subsampling the data
index <- sample(1:nrow(SL), (round(nrow(SL)* 0.01)))
SLsample <- SL[index, ]

###Consider making a training and testing dataset
###This can reduce the computational burden
### It also is a robust goodness of fit method
#SLsample <- SLsample %>% dplyr::mutate(id = row_number())
#Create training set
#train <- SLsample %>% sample_frac(.70)
#Create test set
#test  <- anti_join(SLsample, train, by = 'id')
```
Now that we have a sample, we can fit a model. Fitting a model in always uses the same format. You specify the type of model (e.g. lm, glm), then in brackets define the dependent variable followed by the ~ (tilde). After the tilde you name each of the independent variables with + between them. When these are defined, you use a comma and indicate the data and other specification like the model family you want to use (e.g. binomial, poisson, negative binomial)
```{r}
fit <- glm(urbanChg ~ Parks_dist_SL + Rd_dns1km_SL + WaterDist_SL + DEM_SL,data=SLsample,family=binomial())
summary(fit)
```

#### Interpreting the Model


#### Goodness of Fit
Check the fit of the model by calculating the Area under the curve and the ROC.
```{r}
## Loading required package: gplots
library(ROCR)
# plot a ROC curve for a single prediction run
# and color the curve according to cutoff.
pred <- prediction(predict(fit), SLsample$urbanChg)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
```
```{r}
## A simpler way to understand these result is to calculate the
## area under the curve(AUC). The closer this number is to 1, the
## better your model fit
auc_ROCR <- performance(pred, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR
```

#### Using the Model to Predict Likely Locations of Development
Use the predictors to estimate for each pixel the probability of development given our model
```{r}
predicted <- predict(allrasters, fit)
plot(predicted)
```