---
title: "Applied Spatial Analysis"
author: "[Xiuyu Cao](https://github.com/xiuyucao)"
date: "Nov 18, 2023"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---

******************************************************
## Needed Packages
```{r import needed packages, message=F}
library(terra)  # for working with raster data
library(tidyverse)  # for manipulating the data
library(ape)  # for getting autocorrelation coefficient
library(reshape2)  # for reshaping data frames
library(ROCR)  # for getting the ROC curve
```

******************************************************
## Introduction
* Spatial Analysis simplifies the huge amount of detailed information in order to extract the main trends
* Spatial dependency leads to spatial autocorrelation, which violates standard statistical techniques that assume independence among observations
  * Positive Spatial Autocorrelation
  * Negative Spatial Autocorrelation
* spatial regression models capture the relationships and do not suffer from these weaknesses, but are computationally intensive
* It is also appropriate to view spatial dependency as a source of information rather than something to be corrected
* Spatial Autocorrelation Statistics measure and analyze the degree of dependency among observations in a geographic space.


******************************************************
## Salt Lake City
In recent decades, Salt Lake City, Utah, has undergone considerable urban development characterized by substantial population growth, immigration, and heightened housing demand. This expansion has resulted in a noteworthy 17.6 percent rise in impervious urban development from 2002 to 2010.

### Read Data
First read in the data I will be using to analyze the Salt Lake City. They are raster data of the same resolution and extent. Based on that, they can be combined into a list using the function `terra::c()`, and further turned into a data frame.
```{r read data}
# land use data
NLCD_2001 <- rast("../data/lab5/NLCD_2001_SL.tif")
NLCD_2004 <- rast("../data/lab5/NLCD_2004_SL.tif")
NLCD_2006 <- rast("../data/lab5/NLCD_2006_SL.tif")
NLCD_2008 <- rast("../data/lab5/NLCD_2008_SL.tif")
NLCD_2011 <- rast("../data/lab5/NLCD_2011_SL.tif")
NLCD_2013 <- rast("../data/lab5/NLCD_2013_SL.tif")
NLCD_2016 <- rast("../data/lab5/NLCD_2016_SL.tif")
# distance to parks and protected areas (km)
Park_dist <- rast("../data/lab5/Parks_dist_SL.tif")
# road density for a 1 km neighborhood
Rd_dns1km <- rast("../data/lab5/Rd_dns1km_SL.tif")
# distance to water bodies in km (Euclidean)
WaterDist <- rast("../data/lab5/WaterDist_SL.tif")
# elevation
DEM <- rast("../data/lab5/DEM_SL.tif")
# combine the raster layers
allrasters <- c(NLCD_2001, NLCD_2004, NLCD_2006, NLCD_2008, NLCD_2011, NLCD_2013, NLCD_2016, Park_dist, Rd_dns1km, WaterDist, DEM)

allrasters[[1]]  # check the data
plot(allrasters[[1]])  # check the data

# turn it into a data frame
allrasters.df <- allrasters %>%
  as.data.frame(xy=T) %>%  # transform to a data frame
  filter(NLCD_2001_SL != 128)  # remove no data value (stored as 128)
head(allrasters.df)
```

### Statistical Analysis
identify the pixels that were not urban area in 2001 but have changed to urban area by 2016.
```{r}
allrasters.df <- allrasters.df %>%  # get urbanChg: whether changed to urban area from 2001 to 2016
  mutate(urbanChg = (NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) & (NLCD_2016_SL == 21 | NLCD_2016_SL == 22  | NLCD_2016_SL == 23 | NLCD_2016_SL == 24))
# plot
ggplot(allrasters.df, aes(y=y, x=x, color=urbanChg)) +
   geom_point(size=2, shape=15) +
   theme()
```

**********************************************************************
How much change did Salt Lake experience from 2001 to 2016:
`as.numeric()` converts the logical vector obtained in the previous step to a numeric vector, where TRUE is converted to 1 and FALSE is converted to 0.
`sum()` calculates the sum of the numeric vector obtained in the previous step
```{r}
## calculate total new urban impervious for 2016
newUrban <- with(allrasters.df,(sum(as.numeric(NLCD_2016_SL == 21 | NLCD_2016_SL == 22 | NLCD_2016_SL == 23 | NLCD_2016_SL == 24))) - (sum(as.numeric(NLCD_2001_SL == 21| NLCD_2001_SL == 22| NLCD_2001_SL == 23| NLCD_2001_SL == 24))))
## calculate total urban impervious for 2001
urban2001 <- with(allrasters.df,(sum(as.numeric(NLCD_2001_SL == 21| NLCD_2001_SL == 22| NLCD_2001_SL == 23| NLCD_2001_SL == 24))))
## percentage increase in urban impervious
newUrban/urban2001* 100
```

compare means and plot the variance between newly developed urban areas and the different variables. Let’s look at the influence of the distance to protected areas.

Let’s look at the mean and variance of the variables that might shape where there is new development. We have to reshape the data to achieve this.
```{r}
sl_long <- allrasters.df %>%
  filter(NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) %>%  # get not urban area
  select(Parks_dist_SL:urbanChg) %>%  # select the last columns
  melt()  # turn into long format with urbanChg, varable, and value

# plot
ggplot(sl_long, aes(x=urbanChg, y=value,fill=variable))+
  geom_boxplot()+
  facet_grid(.~variable)+
  labs(x="X (binned)")+
  theme(axis.text.x=element_text(angle=-90, vjust=0.4,hjust=1))
```

### Generalized Linear Models
#### get sample
```{r show how many rows}
nrow(allrasters.df)  # get the row number of the data frame
```
Because of the huge amount of data, a random sample is needed to do the further analysis.

take a random sample of our the entire data set. The sample() function draw a random number indexes representing the rows of a dataframe. From this random sample, we can grab the actual data using the row and column conventions in r . In cases were we have many absence data (area that were not develop) and few presence (developed), a common procedure is to retain all presence data and taking a smaller randomly sample of the absence. A ratio of 1:2 is common, but this is not a rule, and other ratios can be used and are often tested.
```{r set random seed, echo=F}
set.seed(77)
# set.seed(NULL)
```

```{r get random sample of the whole data set}
## --- sample all of the area changed to urban and some of the not changed, make them 1:2
# get all of the area that are not urban in 2001
sl_urban <- filter(allrasters.df,
                   NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24)
# separately get the area changed and not changed to urban
sl_chg <- filter(sl_urban, urbanChg == T)
sl_nchg <- filter(sl_urban, urbanChg == F)
# sample
sample_index_nchg <- sample(1:nrow(sl_nchg), nrow(sl_chg)* 2)
sl_sample <- rbind(sl_chg, sl_nchg[sample_index_nchg,])
```
#### Assess sample
To assess my sampling result, first check the histogram of the original data and the sampled data.
```{r compare histogram}
par(mfrow=c(2, 5),  # set 2*5 sub plots
    mar=c(4, 4, 0.8, 0.65))  # set bottom, left, top, right margin

title=c('LandUse16','Park Dist', 'Road Dense', 'Water Dist', 'DEM')  # set title

for (i in 9:13) {  # plot orignal on the first row
  hist(allrasters.df[, i], main=title[i-8], xlab=NA, col='plum1', ylab=ifelse(i==9,'Frequency (Original)',NA))
}
for (i in 9:13){  # plot sampled on the second row
  hist(sl_sample[, i], main=NA, xlab="Value", col='skyblue', ylab=ifelse(i==9,'Frequency (Sampled)',NA))
}

par(mfrow=c(1, 1))  # set graphic parameter to normal
```

Also, I want to check the spatial autocorrelation. To assess spatial dependency. First get distance matrix, then assess the specific spatial autocorrelation using Moran’s I, which is the most widely use.
```{r assess spatial dependency}
sd_sample <- sl_sample[sample(1:nrow(sl_sample), 100),]
dist_mat <- as.matrix(dist(cbind(sd_sample$x, sd_sample$y)))  # get distance matrix
dist_mat.i <- 1/dist_mat  # get reciprocal distance matrix
diag(dist_mat.i) <- 0  # set diagonal to 0

raster_names <- names(sd_sample)[3:ncol(sd_sample)]
raster_names <- data.frame(Raster_Name = raster_names)
Moran_res <- data.frame()
for(i in 3:ncol(sd_sample)){
  Moran_res <- rbind(Moran_res, Moran.I(sd_sample[,i], dist_mat.i))
}
cbind(raster_names, Moran_res)
```
From the result, the expected value is -0.01 while the observed value is 0.19, indicating the moderate positive spatial autocorrelation. Similar values are clustered together in space

A Moran's I value of 0.19 indicates positive spatial autocorrelation, meaning that similar values are clustered together in space. The Moran's I statistic ranges from -1 to 1, where:

If Moran's I is close to 1, it suggests strong positive spatial autocorrelation.
If Moran's I is close to -1, it suggests strong negative spatial autocorrelation.
If Moran's I is close to 0, it suggests spatial randomness.
In your case, with a Moran's I value of 0.19, the positive sign indicates that nearby locations tend to have similar values. The magnitude (0.19) suggests a moderate degree of spatial autocorrelation. Researchers or analysts would typically interpret this value in the context of the specific study and the characteristics of the data. It implies that there is some level of clustering or pattern in the spatial distribution of the variable being analyzed.

#### train and assess model
Now that we have a sample, we can fit a model. Fitting a model in always uses the same format. You specify the type of model (e.g. lm, glm), then in brackets define the dependent variable followed by the ~ (tilde). After the tilde you name each of the independent variables with + between them. When these are defined, you use a comma and indicate the data and other specification like the model family you want to use (e.g. binomial, poisson, negative binomial)

specifies family=binomial() to indicate that it's fitting a logistic regression model.
logistic Regression:
Use Case: Logistic regression is used when the dependent variable is binary (two categories: 0 or 1).

```{r}
# get train (70%) and test (30%) set
sl_sample <- sl_sample %>% mutate(id = row_number())  # add row number
sl_train <- sl_sample %>% sample_frac(.7)  # get train set
sl_test <- anti_join(sl_sample, sl_train, by='id')  # get test set

fit <- glm(urbanChg ~ Parks_dist_SL + Rd_dns1km_SL + WaterDist_SL + DEM_SL, data=sl_train, family=binomial())
summary(fit)
```

#### Goodness of Fit
Check the fit of the model by calculating the Area under the curve and the ROC.
```{r}
# and color the curve according to cutoff.
pred <- prediction(predict(fit, newdata=sl_test), sl_test$urbanChg)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE, main = "ROC Curve", sub = "Receiver Operating Characteristic")
```
```{r get AUC}
## A simpler way to understand these result is to calculate the
## area under the curve(AUC). The closer this number is to 1, the
## better your model fit
auc_ROCR <- performance(pred, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR
```

#### Using the Model to Predict Likely Locations of Development
Use the predictors to estimate for each pixel the probability of development given our model
```{r predict}
fit <- glm(urbanChg ~ Parks_dist_SL + Rd_dns1km_SL + WaterDist_SL + DEM_SL, data=sl_sample, family=binomial())
predicted <- predict(allrasters, fit)

test <- predicted %>%
  as.data.frame(xy=T)  # transform to a data frame
  
test$lyr1 <- plogis(test$lyr1)

ggplot(test, aes(y=y, x=x, color=lyr1)) +
   geom_point(size=2, shape=15) +
   theme()
```